import numpy as np
import torch
from typing import Dict, Tuple
from typing import Union
from sklearn.decomposition import PCA

# ================================
# ì„¤ì •ê°’
# ================================
K = 3  # Nearest Neighbors ìˆ˜
USE_GPU = torch.cuda.is_available()
DEVICE = torch.device("cuda" if USE_GPU else "cpu")

# ================================
# Step 1: Normal DB êµ¬ì„±
# ================================
def build_normal_database(train_X, train_Y):
    normal_mask = (train_Y == 0)
    normal_db = torch.tensor(train_X[normal_mask], dtype=torch.float32).to(DEVICE)
    return normal_db

# ================================
# Step 2: ê±°ë¦¬ ê³„ì‚° í•¨ìˆ˜
# ================================
def compute_distances(fault_input, normal_db):
    diff = normal_db - fault_input.unsqueeze(0)  # (N_normal, 50, 11)
    distances = torch.mean(diff ** 2, dim=(1, 2))  # (N_normal,)
    return distances

# ================================
# Step 3: ë³´ì • í•¨ìˆ˜
# ================================
def normalize_by_knn(fault_input, normal_db, k=3, method='mean'):
    distances = compute_distances(fault_input, normal_db)
    topk_indices = torch.topk(distances, k=k, largest=False).indices  # kê°œ ì„ íƒ
    topk_seqs = normal_db[topk_indices]  # shape: (k, 50, 11)
    if method == 'mean':
        return torch.mean(topk_seqs, dim=0)
    elif method == 'first':
        return topk_seqs[0]
    else:
        raise ValueError("ì§€ì›ë˜ì§€ ì•ŠëŠ” methodì…ë‹ˆë‹¤: mean / first ì¤‘ ì„ íƒí•˜ì„¸ìš”.")

# ================================
# ì „ì²´ íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ
# ================================
def knn_normalize_batch(test_X, normal_db, k=3, method='mean'):
    test_X_tensor = torch.tensor(test_X, dtype=torch.float32).to(DEVICE)
    normalized_list = []
    for i in range(test_X_tensor.shape[0]):
        normalized = normalize_by_knn(test_X_tensor[i], normal_db, k=k, method=method)
        normalized_list.append(normalized.cpu().numpy())
    return np.stack(normalized_list, axis=0)

class Model2Module:
    """
    Model2: ê³ ì¥ ì‹œì  ì´í›„ ì •ìƒí™” (KNN ê¸°ë°˜, MSE ì‚¬ìš©)
    + Faultë³„ ì •ìƒ-ê³ ì¥ í‰ê·  ê±°ë¦¬ ë¶„ì„ ì§€ì›
    """

    def __init__(self, normal_db: np.ndarray):
        """
        Args:
            normal_db: (N_normal, 50, 11) í˜•íƒœì˜ ì •ìƒ ì‹œí€€ìŠ¤ DB (np.ndarray ë˜ëŠ” tensor í—ˆìš©)
        """
        if isinstance(normal_db, np.ndarray):
            self.normal_db = torch.tensor(normal_db, dtype=torch.float32).to(DEVICE)
        elif isinstance(normal_db, torch.Tensor):
            self.normal_db = normal_db.to(DEVICE)
        else:
            raise TypeError("normal_dbëŠ” numpy.ndarray ë˜ëŠ” torch.Tensor ì—¬ì•¼ í•©ë‹ˆë‹¤.")

    def normalize_after_fault(self,
                              input_sequence: np.ndarray,
                              fault_time: int,
                              k: int = 3,
                              method: str = 'mean') -> Tuple[np.ndarray, Dict]:
        if fault_time is None or fault_time >= input_sequence.shape[0]:
            print("[Model2] ê³ ì¥ ì‹œì ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì…ë ¥ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return input_sequence, {'normalized': False}

        post_fault_input = input_sequence[fault_time:]
        normalized = knn_normalize_batch(
            post_fault_input,
            self.normal_db,
            k=k,
            method=method
        )

        full_output = np.concatenate([
            input_sequence[:fault_time],
            normalized
        ], axis=0)

        return full_output, {
            'normalized': True,
            'fault_time': fault_time,
            'num_normalized_windows': len(post_fault_input)
        }

    def analyze_fault_distance(self, 
                           fault_data: np.ndarray,
                           normal_data: np.ndarray = None) -> float:
        """
        Fault í‰ê·  ì‹œí€€ìŠ¤ì™€ Normal í‰ê·  ì‹œí€€ìŠ¤ ê°„ì˜ MSE ê±°ë¦¬ ê³„ì‚°
        """
        if normal_data is None:
            normal_data = self.normal_db.cpu().numpy()  # ğŸ”§ ìˆ˜ì •: Tensor â†’ numpy

        mean_fault = np.mean(fault_data, axis=0)   # (50, 11)
        mean_normal = np.mean(normal_data, axis=0) # (50, 11)

        mse = np.mean((mean_fault - mean_normal) ** 2)
        return mse


    def analyze_all_faults(self, fault_db: Dict[int, np.ndarray]) -> Dict[int, float]:
        results = {}
        for fault, data in fault_db.items():
            dist = self.analyze_fault_distance(data)
            results[fault] = dist
        return results
    
    
class PCABasedNormalizer:
    def __init__(self, normal_db: np.ndarray, n_components: int = 5):
        """
        PCA ê¸°ë°˜ ë³´ì •ê¸° ì´ˆê¸°í™”
        Args:
            normal_db (np.ndarray): ì •ìƒ DB, shape (N, T, D)
            n_components (int): ì°¨ì› ì¶•ì†Œí•  ì£¼ì„±ë¶„ ìˆ˜
        """
        N, T, D = normal_db.shape
        self.n_components = n_components
        self.T = T
        self.D = D

        # ê° ì‹œì ë³„ PCA ìˆ˜í–‰
        self.pca_models = []
        self.normal_proj = []
        for t in range(T):
            pca = PCA(n_components=n_components)
            X_t = normal_db[:, t, :]
            pca.fit(X_t)
            proj = pca.transform(X_t)
            self.pca_models.append(pca)
            self.normal_proj.append(proj)
        self.normal_proj = np.array(self.normal_proj)  # (T, N, n_components)

    def project(self, seq: np.ndarray) -> np.ndarray:
        """
        ì‹œí€€ìŠ¤ë¥¼ PCA ê³µê°„ìœ¼ë¡œ íˆ¬ì˜
        Args:
            seq (np.ndarray): shape (T, D)
        Returns:
            projected (np.ndarray): shape (T, n_components)
        """
        projected = np.zeros((self.T, self.n_components))
        for t in range(self.T):
            projected[t] = self.pca_models[t].transform(seq[t].reshape(1, -1))[0]
        return projected

    def compute_distances(self, projected_seq: np.ndarray) -> np.ndarray:
        """
        PCA íˆ¬ì˜ ê³µê°„ì—ì„œ ê±°ë¦¬ ê³„ì‚° (MSE)
        Args:
            projected_seq (np.ndarray): shape (T, n_components)
        Returns:
            distances (np.ndarray): shape (N,)
        """
        diff = self.normal_proj - projected_seq[None, :, :]  # (N, T, n_components)
        distances = np.mean(diff ** 2, axis=(1, 2))  # (N,)
        return distances
    
    def knn_normalize(self,
                      fault_sequence: np.ndarray,
                      k: int = 3,
                      method: str = 'mean',
                      return_pca_vector: bool = False
                      ) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:
        assert fault_sequence.shape == (self.T, self.D)

        normalized_sequence = []
        corrected_pca_sequence = []

        for t in range(self.T):
            pca = self.pca_models[t]
            normal_proj_t = self.normal_proj[:, t, :]  # (N, n_components)
            fault_proj_t = pca.transform(fault_sequence[t].reshape(1, -1))  # (1, n_components)

            dists = np.linalg.norm(normal_proj_t - fault_proj_t, axis=1)
            topk_indices = np.argsort(dists)[:k]
            topk_proj = normal_proj_t[topk_indices]

            if method == 'mean':
                corrected_proj = np.mean(topk_proj, axis=0)
            elif method == 'first':
                corrected_proj = topk_proj[0]
            else:
                raise ValueError("ì§€ì›ë˜ì§€ ì•ŠëŠ” methodì…ë‹ˆë‹¤: 'mean' ë˜ëŠ” 'first'")

            reconstructed = pca.inverse_transform(corrected_proj.reshape(1, -1)).reshape(-1)

            normalized_sequence.append(reconstructed)
            corrected_pca_sequence.append(corrected_proj)

        normalized_sequence = np.stack(normalized_sequence, axis=0)
        corrected_pca_sequence = np.stack(corrected_pca_sequence, axis=0)

        if return_pca_vector:
            return normalized_sequence, corrected_pca_sequence
        else:
            return normalized_sequence